{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling of Lyrics with LDA\n",
    "\n",
    "Author: Miles Mezaki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>artist</th>\n",
       "      <th>seq</th>\n",
       "      <th>song</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>No, no\\r\\nI ain't ever trapped out the bando\\r...</td>\n",
       "      <td>Everyday</td>\n",
       "      <td>0.626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>The drinks go down and smoke goes up, I feel m...</td>\n",
       "      <td>Live Till We Die</td>\n",
       "      <td>0.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>She don't live on planet Earth no more\\r\\nShe ...</td>\n",
       "      <td>The Otherside</td>\n",
       "      <td>0.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>Trippin' off that Grigio, mobbin', lights low\\...</td>\n",
       "      <td>Pinot</td>\n",
       "      <td>0.536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>I see a midnight panther, so gallant and so br...</td>\n",
       "      <td>Shadows &amp; Diamonds</td>\n",
       "      <td>0.371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158348</th>\n",
       "      <td>158348</td>\n",
       "      <td>Adam Green</td>\n",
       "      <td>And we live on borrowed time,\\r\\nBut this head...</td>\n",
       "      <td>Friends of Mine</td>\n",
       "      <td>0.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158349</th>\n",
       "      <td>158349</td>\n",
       "      <td>Adam Green</td>\n",
       "      <td>Frozin in time forever\\r\\nCarrying that torch ...</td>\n",
       "      <td>Frozen in Time</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158350</th>\n",
       "      <td>158350</td>\n",
       "      <td>Adam Green</td>\n",
       "      <td>Hard to be a girl. \\r\\nSo nice to be a boy. \\r...</td>\n",
       "      <td>Hard to Be a Girl</td>\n",
       "      <td>0.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158351</th>\n",
       "      <td>158351</td>\n",
       "      <td>Adam Green</td>\n",
       "      <td>I want to chose to die,\\r\\nAnd be buried with ...</td>\n",
       "      <td>I Wanna Die</td>\n",
       "      <td>0.361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158352</th>\n",
       "      <td>158352</td>\n",
       "      <td>Adam Green</td>\n",
       "      <td>Musical ladders\\r\\nLeaning on mountains\\r\\nBat...</td>\n",
       "      <td>Musical Ladders</td>\n",
       "      <td>0.263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158353 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0        artist  \\\n",
       "0                0  Elijah Blake   \n",
       "1                1  Elijah Blake   \n",
       "2                2  Elijah Blake   \n",
       "3                3  Elijah Blake   \n",
       "4                4  Elijah Blake   \n",
       "...            ...           ...   \n",
       "158348      158348    Adam Green   \n",
       "158349      158349    Adam Green   \n",
       "158350      158350    Adam Green   \n",
       "158351      158351    Adam Green   \n",
       "158352      158352    Adam Green   \n",
       "\n",
       "                                                      seq                song  \\\n",
       "0       No, no\\r\\nI ain't ever trapped out the bando\\r...            Everyday   \n",
       "1       The drinks go down and smoke goes up, I feel m...    Live Till We Die   \n",
       "2       She don't live on planet Earth no more\\r\\nShe ...       The Otherside   \n",
       "3       Trippin' off that Grigio, mobbin', lights low\\...               Pinot   \n",
       "4       I see a midnight panther, so gallant and so br...  Shadows & Diamonds   \n",
       "...                                                   ...                 ...   \n",
       "158348  And we live on borrowed time,\\r\\nBut this head...     Friends of Mine   \n",
       "158349  Frozin in time forever\\r\\nCarrying that torch ...      Frozen in Time   \n",
       "158350  Hard to be a girl. \\r\\nSo nice to be a boy. \\r...   Hard to Be a Girl   \n",
       "158351  I want to chose to die,\\r\\nAnd be buried with ...         I Wanna Die   \n",
       "158352  Musical ladders\\r\\nLeaning on mountains\\r\\nBat...     Musical Ladders   \n",
       "\n",
       "        label  \n",
       "0       0.626  \n",
       "1       0.630  \n",
       "2       0.240  \n",
       "3       0.536  \n",
       "4       0.371  \n",
       "...       ...  \n",
       "158348  0.737  \n",
       "158349  0.482  \n",
       "158350  0.733  \n",
       "158351  0.361  \n",
       "158352  0.263  \n",
       "\n",
       "[158353 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/labeled_lyrics_cleaned.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define code to expand contracted words, which are prominent in music:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, no\n",
      "I ain't ever trapped out the bando\n",
      "But oh Lord, don't get me wrong\n",
      "I know a couple niggas that do\n",
      "I'm from a place where everybody knows your name\n",
      "They say I gotta watch my attitude\n",
      "When they see money, man they all start actin' strange\n",
      "So fuck with the ones that fuck with you\n",
      "They can never say I'm brand new\n",
      "\n",
      "It's everyday, everyday\n",
      "Everyday, everyday, everyday\n",
      "Everyday, everyday\n",
      "Everyday, everyday\n",
      "I've been talkin' my shit, nigga that's regular\n",
      "I've been lovin' 'em thick, life is spectacular\n",
      "I spend like I'ma die rich, nigga I'm flexin', yeah\n",
      "Everyday, that's everyday\n",
      "That's everyday\n",
      "That's everyday\n",
      "That's everyday, everyday\n",
      "\n",
      "I see all of these wanna-be hot R&B singers\n",
      "I swear you all sound the same\n",
      "They start from the bottom, so far from the motto\n",
      "You niggas'll never be Drake\n",
      "Shout out to OVO\n",
      "Most of them prolly don't know me though\n",
      "I stay in the cut, I don't fuck with no\n",
      "Body but I D, that's a pun on No I.D\n",
      "When nobody know my name\n",
      "Runnin' for my dream wasn't hard to do\n",
      "You break bread, I swear they all pull out a plate\n",
      "Eat with the ones who starved with you\n",
      "If I'm winnin' then my crew can't lose\n",
      "\n",
      "It's everyday, everyday\n",
      "Everyday, everyday, everyday\n",
      "Everyday, everyday\n",
      "Everyday, everyday\n",
      "I've been talkin' my shit, nigga that's regular\n",
      "I've been lovin' 'em thick, life is spectacular\n",
      "I spend like I'ma die rich, nigga I'm flexin', yeah\n",
      "Everyday, that's everyday\n",
      "That's everyday\n",
      "That's everyday\n",
      "That's everyday, everyday\n",
      "\n",
      "I heard since you got money\n",
      "You changed, you're actin' funny\n",
      "That's why I gets on my lonely\n",
      "You be lovin' when change is a hobby\n",
      "Who do you dress when you ain't got nobody?\n",
      "\n",
      "It's everyday, everyday\n",
      "Everyday, everyday, everyday\n",
      "Everyday, everyday\n",
      "Everyday, everyday\n",
      "I've been talkin' my shit, nigga that's regular\n",
      "I've been lovin' 'em thick, life is spectacular\n",
      "I spend like I'ma die rich, nigga I'm flexin', yeah\n",
      "Everyday, that's everyday\n",
      "That's everyday\n",
      "That's everyday\n",
      "That's everyday, everyday\n"
     ]
    }
   ],
   "source": [
    "def expand_contractions(s):\n",
    "    res = \"\"\n",
    "    contractions = { \n",
    "        \"ain't\": \"is not\", # Could be many tenses, but is not is probably the best mapping for all tenses\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"'cause\": \"because\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hadn't've\": \"had not have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"he'd've\": \"he would have\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"he'll've\": \"he will have\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"how'd'y\": \"how do you\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"I'd\": \"I would\",\n",
    "        \"I'd've\": \"I would have\",\n",
    "        \"I'll\": \"I will\",\n",
    "        \"I'll've\": \"I will have\",\n",
    "        \"I'm\": \"I am\",\n",
    "        \"I've\": \"I have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\",\n",
    "        \"it'd've\": \"it would have\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"it'll've\": \"it will have\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mightn't've\": \"might not have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn't've\": \"must not have\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"needn't've\": \"need not have\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"oughtn't've\": \"ought not have\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"sha'n't\": \"shall not\",\n",
    "        \"shan't've\": \"shall not have\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"she'd've\": \"she would have\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"she'll've\": \"she will have\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\",\n",
    "        \"so've\": \"so have\",\n",
    "        \"so's\": \"so is\",\n",
    "        \"that'd\": \"that would\",\n",
    "        \"that'd've\": \"that would have\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"there'd\": \"there would\",\n",
    "        \"there'd've\": \"there would have\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"they'd've\": \"they would have\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"they'll've\": \"they will have\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"to've\": \"to have\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we'll've\": \"we will have\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"what'll\": \"what will\",\n",
    "        \"what'll've\": \"what will have\",\n",
    "        \"what're\": \"what are\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"when's\": \"when is\",\n",
    "        \"when've\": \"when have\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"who'll\": \"who will\",\n",
    "        \"who'll've\": \"who will have\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"who've\": \"who have\",\n",
    "        \"why's\": \"why is\",\n",
    "        \"why've\": \"why have\",\n",
    "        \"will've\": \"will have\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"won't've\": \"will not have\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'all'd\": \"you all would\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all've\": \"you all have\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"you'd've\": \"you would have\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"you'll've\": \"you will have\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\"\n",
    "        }\n",
    "    for word in s.split():\n",
    "        if word in contractions:\n",
    "            res += contractions[word] + \" \"\n",
    "        else:\n",
    "            res += word + \" \"\n",
    "    return res\n",
    "\n",
    "print(df['seq'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No, no I is not ever trapped out the bando But oh Lord, do not get me wrong I know a couple niggas that do I am from a place where everybody knows your name They say I gotta watch my attitude When they see money, man they all start actin' strange So fuck with the ones that fuck with you They can never say I am brand new It's everyday, everyday Everyday, everyday, everyday Everyday, everyday Everyday, everyday I have been talkin' my shit, nigga that is regular I have been lovin' 'em thick, life is spectacular I spend like I'ma die rich, nigga I am flexin', yeah Everyday, that is everyday That's everyday That's everyday That's everyday, everyday I see all of these wanna-be hot R&B singers I swear you all sound the same They start from the bottom, so far from the motto You niggas'll never be Drake Shout out to OVO Most of them prolly do not know me though I stay in the cut, I do not fuck with no Body but I D, that is a pun on No I.D When nobody know my name Runnin' for my dream was not hard to do You break bread, I swear they all pull out a plate Eat with the ones who starved with you If I am winnin' then my crew cannot lose It's everyday, everyday Everyday, everyday, everyday Everyday, everyday Everyday, everyday I have been talkin' my shit, nigga that is regular I have been lovin' 'em thick, life is spectacular I spend like I'ma die rich, nigga I am flexin', yeah Everyday, that is everyday That's everyday That's everyday That's everyday, everyday I heard since you got money You changed, you are actin' funny That's why I gets on my lonely You be lovin' when change is a hobby Who do you dress when you is not got nobody? It's everyday, everyday Everyday, everyday, everyday Everyday, everyday Everyday, everyday I have been talkin' my shit, nigga that is regular I have been lovin' 'em thick, life is spectacular I spend like I'ma die rich, nigga I am flexin', yeah Everyday, that is everyday That's everyday That's everyday That's everyday, everyday \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_contractions(df['seq'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next define code that will match expressions missing the trailing g in their progressive tense and replace the apostrophe with a g. (e.g. trippin' -> tripping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tripping, gripping, dribbling.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"(.)in'\"\n",
    "replacement =  r\"\\1ing\"\n",
    "def expand_verbs(s: str, pattern, replacement):\n",
    "    \"\"\"\n",
    "    Assume s is the lyrics string\n",
    "    \"\"\"\n",
    "    res, _ = re.subn(pattern, replacement, s)\n",
    "    return res\n",
    "\n",
    "expand_verbs(\"trippin', grippin', dribblin'.\", pattern, replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"no, no i is not ever trapped out the bando but oh lord, do not get me wrong i know a couple niggas that do i'm from a place where everybody knows your name they say i gotta watch my attitude when they see money, man they all start acting strange so fuck with the ones that fuck with you they can never say i'm brand new it is everyday, everyday everyday, everyday, everyday everyday, everyday everyday, everyday i've been talking my shit, nigga that is regular i've been loving 'em thick, life is spectacular i spend like i'ma die rich, nigga i'm flexing, yeah everyday, that is everyday that is everyday that is everyday that is everyday, everyday i see all of these wanna-be hot r&b singers i swear you all sound the same they start from the bottom, so far from the motto you niggas'll never be drake shout out to ovo most of them prolly do not know me though i stay in the cut, i do not fuck with no body but i d, that is a pun on no i.d when nobody know my name running for my dream was not hard to do you break bread, i swear they all pull out a plate eat with the ones who starved with you if i'm winning then my crew cannot lose it is everyday, everyday everyday, everyday, everyday everyday, everyday everyday, everyday i've been talking my shit, nigga that is regular i've been loving 'em thick, life is spectacular i spend like i'ma die rich, nigga i'm flexing, yeah everyday, that is everyday that is everyday that is everyday that is everyday, everyday i heard since you got money you changed, you are acting funny that is why i gets on my lonely you be loving when change is a hobby who do you dress when you is not got nobody? it is everyday, everyday everyday, everyday, everyday everyday, everyday everyday, everyday i've been talking my shit, nigga that is regular i've been loving 'em thick, life is spectacular i spend like i'ma die rich, nigga i'm flexing, yeah everyday, that is everyday that is everyday that is everyday that is everyday, everyday \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_verbs_and_contractions(s, pattern=r\"(.)in'\", replacement=r\"\\1ing\"):\n",
    "    \"\"\"\n",
    "    s: string representing lyrics\n",
    "    pattern: regular expression representing the pattern to be replaced\n",
    "    replacement: regular expression to replace pattern\n",
    "\n",
    "    Returns:\n",
    "        res: string ready for tokenization\n",
    "    \"\"\"\n",
    "    return expand_verbs(expand_contractions(s.lower()), pattern, replacement)\n",
    "    \n",
    "expand_verbs_and_contractions(df['seq'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seq'] = df['seq'].apply(expand_verbs_and_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>artist</th>\n",
       "      <th>seq</th>\n",
       "      <th>song</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>no, no i is not ever trapped out the bando but...</td>\n",
       "      <td>Everyday</td>\n",
       "      <td>0.626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>the drinks go down and smoke goes up, i feel m...</td>\n",
       "      <td>Live Till We Die</td>\n",
       "      <td>0.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>she do not live on planet earth no more she fo...</td>\n",
       "      <td>The Otherside</td>\n",
       "      <td>0.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>tripping off that grigio, mobbing, lights low ...</td>\n",
       "      <td>Pinot</td>\n",
       "      <td>0.536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>i see a midnight panther, so gallant and so br...</td>\n",
       "      <td>Shadows &amp; Diamonds</td>\n",
       "      <td>0.371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        artist  \\\n",
       "0           0  Elijah Blake   \n",
       "1           1  Elijah Blake   \n",
       "2           2  Elijah Blake   \n",
       "3           3  Elijah Blake   \n",
       "4           4  Elijah Blake   \n",
       "\n",
       "                                                 seq                song  \\\n",
       "0  no, no i is not ever trapped out the bando but...            Everyday   \n",
       "1  the drinks go down and smoke goes up, i feel m...    Live Till We Die   \n",
       "2  she do not live on planet earth no more she fo...       The Otherside   \n",
       "3  tripping off that grigio, mobbing, lights low ...               Pinot   \n",
       "4  i see a midnight panther, so gallant and so br...  Shadows & Diamonds   \n",
       "\n",
       "   label  \n",
       "0  0.626  \n",
       "1  0.630  \n",
       "2  0.240  \n",
       "3  0.536  \n",
       "4  0.371  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to tokenize and stem the data. We will not remove stop words since the people are generally important in lyrics and the verbs are often stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no',\n",
       " ',',\n",
       " 'no',\n",
       " 'i',\n",
       " 'is',\n",
       " 'not',\n",
       " 'ever',\n",
       " 'trapped',\n",
       " 'out',\n",
       " 'the',\n",
       " 'bando',\n",
       " 'but',\n",
       " 'oh',\n",
       " 'lord',\n",
       " ',',\n",
       " 'do',\n",
       " 'not',\n",
       " 'get',\n",
       " 'me',\n",
       " 'wrong',\n",
       " 'i',\n",
       " 'know',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'niggas',\n",
       " 'that',\n",
       " 'do',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'from',\n",
       " 'a',\n",
       " 'place',\n",
       " 'where',\n",
       " 'everybody',\n",
       " 'knows',\n",
       " 'your',\n",
       " 'name',\n",
       " 'they',\n",
       " 'say',\n",
       " 'i',\n",
       " 'got',\n",
       " 'ta',\n",
       " 'watch',\n",
       " 'my',\n",
       " 'attitude',\n",
       " 'when',\n",
       " 'they',\n",
       " 'see',\n",
       " 'money',\n",
       " ',',\n",
       " 'man',\n",
       " 'they',\n",
       " 'all',\n",
       " 'start',\n",
       " 'acting',\n",
       " 'strange',\n",
       " 'so',\n",
       " 'fuck',\n",
       " 'with',\n",
       " 'the',\n",
       " 'ones',\n",
       " 'that',\n",
       " 'fuck',\n",
       " 'with',\n",
       " 'you',\n",
       " 'they',\n",
       " 'can',\n",
       " 'never',\n",
       " 'say',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'brand',\n",
       " 'new',\n",
       " 'it',\n",
       " 'is',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'been',\n",
       " 'talking',\n",
       " 'my',\n",
       " 'shit',\n",
       " ',',\n",
       " 'nigga',\n",
       " 'that',\n",
       " 'is',\n",
       " 'regular',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'been',\n",
       " 'loving',\n",
       " \"'em\",\n",
       " 'thick',\n",
       " ',',\n",
       " 'life',\n",
       " 'is',\n",
       " 'spectacular',\n",
       " 'i',\n",
       " 'spend',\n",
       " 'like',\n",
       " \"i'ma\",\n",
       " 'die',\n",
       " 'rich',\n",
       " ',',\n",
       " 'nigga',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'flexing',\n",
       " ',',\n",
       " 'yeah',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'that',\n",
       " 'is',\n",
       " 'everyday',\n",
       " 'that',\n",
       " 'is',\n",
       " 'everyday',\n",
       " 'that',\n",
       " 'is',\n",
       " 'everyday',\n",
       " 'that',\n",
       " 'is',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday',\n",
       " 'i',\n",
       " 'see',\n",
       " 'all',\n",
       " 'of',\n",
       " 'these',\n",
       " 'wanna-be',\n",
       " 'hot',\n",
       " 'r',\n",
       " '&',\n",
       " 'b',\n",
       " 'singers',\n",
       " 'i',\n",
       " 'swear',\n",
       " 'you',\n",
       " 'all',\n",
       " 'sound',\n",
       " 'the',\n",
       " 'same',\n",
       " 'they',\n",
       " 'start',\n",
       " 'from',\n",
       " 'the',\n",
       " 'bottom',\n",
       " ',',\n",
       " 'so',\n",
       " 'far',\n",
       " 'from',\n",
       " 'the',\n",
       " 'motto',\n",
       " 'you',\n",
       " 'niggas',\n",
       " \"'ll\",\n",
       " 'never',\n",
       " 'be',\n",
       " 'drake',\n",
       " 'shout',\n",
       " 'out',\n",
       " 'to',\n",
       " 'ovo',\n",
       " 'most',\n",
       " 'of',\n",
       " 'them',\n",
       " 'prolly',\n",
       " 'do',\n",
       " 'not',\n",
       " 'know',\n",
       " 'me',\n",
       " 'though',\n",
       " 'i',\n",
       " 'stay',\n",
       " 'in',\n",
       " 'the',\n",
       " 'cut',\n",
       " ',',\n",
       " 'i',\n",
       " 'do',\n",
       " 'not',\n",
       " 'fuck',\n",
       " 'with',\n",
       " 'no',\n",
       " 'body',\n",
       " 'but',\n",
       " 'i',\n",
       " 'd',\n",
       " ',',\n",
       " 'that',\n",
       " 'is',\n",
       " 'a',\n",
       " 'pun',\n",
       " 'on',\n",
       " 'no',\n",
       " 'i.d',\n",
       " 'when',\n",
       " 'nobody',\n",
       " 'know',\n",
       " 'my',\n",
       " 'name',\n",
       " 'running',\n",
       " 'for',\n",
       " 'my',\n",
       " 'dream',\n",
       " 'was',\n",
       " 'not',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'do',\n",
       " 'you',\n",
       " 'break',\n",
       " 'bread',\n",
       " ',',\n",
       " 'i',\n",
       " 'swear',\n",
       " 'they',\n",
       " 'all',\n",
       " 'pull',\n",
       " 'out',\n",
       " 'a',\n",
       " 'plate',\n",
       " 'eat',\n",
       " 'with',\n",
       " 'the',\n",
       " 'ones',\n",
       " 'who',\n",
       " 'starved',\n",
       " 'with',\n",
       " 'you',\n",
       " 'if',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'winning',\n",
       " 'then',\n",
       " 'my',\n",
       " 'crew',\n",
       " 'can',\n",
       " 'not',\n",
       " 'lose',\n",
       " 'it',\n",
       " 'is',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'been',\n",
       " 'talking',\n",
       " 'my',\n",
       " 'shit',\n",
       " ',',\n",
       " 'nigga',\n",
       " 'that',\n",
       " 'is',\n",
       " 'regular',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'been',\n",
       " 'loving',\n",
       " \"'em\",\n",
       " 'thick',\n",
       " ',',\n",
       " 'life',\n",
       " 'is',\n",
       " 'spectacular',\n",
       " 'i',\n",
       " 'spend',\n",
       " 'like',\n",
       " \"i'ma\",\n",
       " 'die',\n",
       " 'rich',\n",
       " ',',\n",
       " 'nigga',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'flexing',\n",
       " ',',\n",
       " 'yeah',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'that',\n",
       " 'is',\n",
       " 'everyday',\n",
       " 'that',\n",
       " 'is',\n",
       " 'everyday',\n",
       " 'that',\n",
       " 'is',\n",
       " 'everyday',\n",
       " 'that',\n",
       " 'is',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday',\n",
       " 'i',\n",
       " 'heard',\n",
       " 'since',\n",
       " 'you',\n",
       " 'got',\n",
       " 'money',\n",
       " 'you',\n",
       " 'changed',\n",
       " ',',\n",
       " 'you',\n",
       " 'are',\n",
       " 'acting',\n",
       " 'funny',\n",
       " 'that',\n",
       " 'is',\n",
       " 'why',\n",
       " 'i',\n",
       " 'gets',\n",
       " 'on',\n",
       " 'my',\n",
       " 'lonely',\n",
       " 'you',\n",
       " 'be',\n",
       " 'loving',\n",
       " 'when',\n",
       " 'change',\n",
       " 'is',\n",
       " 'a',\n",
       " 'hobby',\n",
       " 'who',\n",
       " 'do',\n",
       " 'you',\n",
       " 'dress',\n",
       " 'when',\n",
       " 'you',\n",
       " 'is',\n",
       " 'not',\n",
       " 'got',\n",
       " 'nobody',\n",
       " '?',\n",
       " 'it',\n",
       " 'is',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'been',\n",
       " 'talking',\n",
       " 'my',\n",
       " 'shit',\n",
       " ',',\n",
       " 'nigga',\n",
       " 'that',\n",
       " 'is',\n",
       " 'regular',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'been',\n",
       " 'loving',\n",
       " \"'em\",\n",
       " 'thick',\n",
       " ',',\n",
       " 'life',\n",
       " 'is',\n",
       " 'spectacular',\n",
       " 'i',\n",
       " 'spend',\n",
       " 'like',\n",
       " \"i'ma\",\n",
       " 'die',\n",
       " 'rich',\n",
       " ',',\n",
       " 'nigga',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'flexing',\n",
       " ',',\n",
       " 'yeah',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'that',\n",
       " 'is',\n",
       " 'everyday',\n",
       " 'that',\n",
       " 'is',\n",
       " 'everyday',\n",
       " 'that',\n",
       " 'is',\n",
       " 'everyday',\n",
       " 'that',\n",
       " 'is',\n",
       " 'everyday',\n",
       " ',',\n",
       " 'everyday']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "text = df['seq'][0]\n",
    "# tokenize\n",
    "result = word_tokenize(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tripping  :  trip\n",
      "off  :  off\n",
      "that  :  that\n",
      "grigio,  :  grigio,\n",
      "mobbing,  :  mobbing,\n",
      "lights  :  light\n",
      "low  :  low\n",
      "tripping  :  trip\n",
      "off  :  off\n",
      "that  :  that\n",
      "grigio,  :  grigio,\n",
      "mobbing,  :  mobbing,\n",
      "lights  :  light\n",
      "low  :  low\n",
      "tripping  :  trip\n",
      "off  :  off\n",
      "that  :  that\n",
      "grigio,  :  grigio,\n",
      "mobbing,  :  mobbing,\n",
      "lights  :  light\n",
      "low  :  low\n",
      "baby  :  babi\n",
      "i'm  :  i'm\n",
      "surprised  :  surpris\n",
      "you  :  you\n",
      "picked  :  pick\n",
      "up  :  up\n",
      "this  :  thi\n",
      "late  :  late\n",
      "at  :  at\n",
      "night  :  night\n",
      "and  :  and\n",
      "i'm  :  i'm\n",
      "not  :  not\n",
      "tryna  :  tryna\n",
      "start  :  start\n",
      "a  :  a\n",
      "fight  :  fight\n",
      "but  :  but\n",
      "i  :  i\n",
      "hate  :  hate\n",
      "the  :  the\n",
      "way  :  way\n",
      "we  :  we\n",
      "ended  :  end\n",
      "and  :  and\n",
      "the  :  the\n",
      "fact  :  fact\n",
      "you  :  you\n",
      "got  :  got\n",
      "me  :  me\n",
      "cheated  :  cheat\n",
      "i  :  i\n",
      "hate  :  hate\n",
      "these  :  these\n",
      "new  :  new\n",
      "beginnings  :  begin\n",
      "even  :  even\n",
      "though  :  though\n",
      "i  :  i\n",
      "am  :  am\n",
      "the  :  the\n",
      "reason  :  reason\n",
      "the  :  the\n",
      "only  :  onli\n",
      "thing  :  thing\n",
      "i  :  i\n",
      "love  :  love\n",
      "right  :  right\n",
      "now  :  now\n",
      "is  :  is\n",
      "you  :  you\n",
      "i  :  i\n",
      "miss  :  miss\n",
      "ya  :  ya\n",
      "going  :  go\n",
      "through  :  through\n",
      "my  :  my\n",
      "phone  :  phone\n",
      "and  :  and\n",
      "calling  :  call\n",
      "up  :  up\n",
      "the  :  the\n",
      "numbers  :  number\n",
      "you  :  you\n",
      "do  :  do\n",
      "not  :  not\n",
      "know  :  know\n",
      "the  :  the\n",
      "make  :  make\n",
      "up  :  up\n",
      "sex,  :  sex,\n",
      "even  :  even\n",
      "when  :  when\n",
      "it  :  it\n",
      "is  :  is\n",
      "wrong  :  wrong\n",
      "our  :  our\n",
      "love  :  love\n",
      "was  :  wa\n",
      "that  :  that\n",
      "strong  :  strong\n",
      "[hook  :  [hook\n",
      "1]  :  1]\n",
      "i'm  :  i'm\n",
      "just  :  just\n",
      "tripping  :  trip\n",
      "off  :  off\n",
      "that  :  that\n",
      "grigio  :  grigio\n",
      "it  :  it\n",
      "sucks  :  suck\n",
      "to  :  to\n",
      "see  :  see\n",
      "that  :  that\n",
      "you  :  you\n",
      "do  :  do\n",
      "not  :  not\n",
      "feel  :  feel\n",
      "me  :  me\n",
      "though  :  though\n",
      "pour  :  pour\n",
      "me  :  me\n",
      "a  :  a\n",
      "glass  :  glass\n",
      "and  :  and\n",
      "let  :  let\n",
      "my  :  my\n",
      "feelings  :  feel\n",
      "blow  :  blow\n",
      "hell  :  hell\n",
      "naw,  :  naw,\n",
      "this  :  thi\n",
      "message  :  messag\n",
      "is  :  is\n",
      "not  :  not\n",
      "subliminal  :  sublimin\n",
      "aw  :  aw\n",
      "baby,  :  baby,\n",
      "pour  :  pour\n",
      "me  :  me\n",
      "a  :  a\n",
      "drink  :  drink\n",
      "to  :  to\n",
      "get  :  get\n",
      "over  :  over\n",
      "you  :  you\n",
      "aw  :  aw\n",
      "baby,  :  baby,\n",
      "aw  :  aw\n",
      "baby,  :  baby,\n",
      "i  :  i\n",
      "is  :  is\n",
      "not  :  not\n",
      "no  :  no\n",
      "drinker  :  drinker\n",
      "but  :  but\n",
      "it  :  it\n",
      "is  :  is\n",
      "getting  :  get\n",
      "me  :  me\n",
      "though  :  though\n",
      "tripping  :  trip\n",
      "off  :  off\n",
      "that  :  that\n",
      "grigio,  :  grigio,\n",
      "mobbing,  :  mobbing,\n",
      "lights  :  light\n",
      "low  :  low\n",
      "tripping  :  trip\n",
      "off  :  off\n",
      "that  :  that\n",
      "grigio,  :  grigio,\n",
      "mobbing,  :  mobbing,\n",
      "lights  :  light\n",
      "low  :  low\n",
      "tripping  :  trip\n",
      "off  :  off\n",
      "that  :  that\n",
      "grigio,  :  grigio,\n",
      "mobbing,  :  mobbing,\n",
      "lights  :  light\n",
      "low  :  low\n",
      "tripping  :  trip\n",
      "off  :  off\n",
      "that  :  that\n",
      "grigio,  :  grigio,\n",
      "mobbing  :  mob\n",
      "i  :  i\n",
      "do  :  do\n",
      "not  :  not\n",
      "know  :  know\n",
      "why  :  whi\n",
      "your  :  your\n",
      "father  :  father\n",
      "never  :  never\n",
      "like  :  like\n",
      "me  :  me\n",
      "was  :  wa\n",
      "it  :  it\n",
      "because  :  becaus\n",
      "of  :  of\n",
      "my  :  my\n",
      "[?]boy  :  [?]boy\n",
      "ways  :  way\n",
      "and  :  and\n",
      "this  :  thi\n",
      "white  :  white\n",
      "tee?  :  tee?\n",
      "your  :  your\n",
      "eye  :  eye\n",
      "always  :  alway\n",
      "thought  :  thought\n",
      "your  :  your\n",
      "momma  :  momma\n",
      "was  :  wa\n",
      "a  :  a\n",
      "winner  :  winner\n",
      "considering  :  consid\n",
      "her  :  her\n",
      "only  :  onli\n",
      "stayed  :  stay\n",
      "together  :  togeth\n",
      "after  :  after\n",
      "all  :  all\n",
      "those  :  those\n",
      "years  :  year\n",
      "you  :  you\n",
      "think  :  think\n",
      "that  :  that\n",
      "we  :  we\n",
      "could  :  could\n",
      "learn  :  learn\n",
      "from  :  from\n",
      "them  :  them\n",
      "i  :  i\n",
      "miss  :  miss\n",
      "ya  :  ya\n",
      "going  :  go\n",
      "through  :  through\n",
      "my  :  my\n",
      "phone  :  phone\n",
      "and  :  and\n",
      "calling  :  call\n",
      "up  :  up\n",
      "the  :  the\n",
      "numbers  :  number\n",
      "you  :  you\n",
      "do  :  do\n",
      "not  :  not\n",
      "know  :  know\n",
      "the  :  the\n",
      "make  :  make\n",
      "up  :  up\n",
      "sex,  :  sex,\n",
      "even  :  even\n",
      "when  :  when\n",
      "it  :  it\n",
      "is  :  is\n",
      "wrong  :  wrong\n",
      "our  :  our\n",
      "love  :  love\n",
      "was  :  wa\n",
      "that  :  that\n",
      "strong  :  strong\n",
      "nights  :  night\n",
      "like  :  like\n",
      "this,  :  this,\n",
      "we  :  we\n",
      "should  :  should\n",
      "be  :  be\n",
      "tripping  :  trip\n",
      "off  :  off\n",
      "that  :  that\n",
      "grigio  :  grigio\n",
      "telling  :  tell\n",
      "me  :  me\n",
      "secrets  :  secret\n",
      "that  :  that\n",
      "nobody  :  nobodi\n",
      "knows  :  know\n",
      "pour  :  pour\n",
      "us  :  us\n",
      "a  :  a\n",
      "shot  :  shot\n",
      "and  :  and\n",
      "cut  :  cut\n",
      "the  :  the\n",
      "music  :  music\n",
      "on  :  on\n",
      "now  :  now\n",
      "let  :  let\n",
      "us  :  us\n",
      "get  :  get\n",
      "started,  :  started,\n",
      "cut  :  cut\n",
      "these  :  these\n",
      "fucking  :  fuck\n",
      "lights  :  light\n",
      "off  :  off\n",
      "aw  :  aw\n",
      "baby,  :  baby,\n",
      "i'ma  :  i'ma\n",
      "pour  :  pour\n",
      "a  :  a\n",
      "drink  :  drink\n",
      "for  :  for\n",
      "you  :  you\n",
      "aw  :  aw\n",
      "baby,  :  baby,\n",
      "aw  :  aw\n",
      "baby,  :  baby,\n",
      "i'ma  :  i'ma\n",
      "pour  :  pour\n",
      "a  :  a\n",
      "drink  :  drink\n",
      "for  :  for\n",
      "you  :  you\n",
      "tripping  :  trip\n",
      "off  :  off\n",
      "that  :  that\n",
      "grigio,  :  grigio,\n",
      "mobbing,  :  mobbing,\n",
      "lights  :  light\n",
      "low  :  low\n",
      "tripping  :  trip\n",
      "off  :  off\n",
      "that  :  that\n",
      "grigio,  :  grigio,\n",
      "mobbing,  :  mobbing,\n",
      "lights  :  light\n",
      "low  :  low\n",
      "tripping  :  trip\n",
      "off  :  off\n",
      "that  :  that\n",
      "grigio,  :  grigio,\n",
      "mobbing,  :  mobbing,\n",
      "lights  :  light\n",
      "low  :  low\n",
      "tripping  :  trip\n",
      "off  :  off\n",
      "that  :  that\n",
      "grigio,  :  grigio,\n",
      "mobbing  :  mob\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    " \n",
    "ps = PorterStemmer()\n",
    " \n",
    "# choose some words to be stemmed\n",
    "words = df['seq'][3]\n",
    " \n",
    "for w in words.split():\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'convert the word in word_token to lower case and then check whether'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def token_stem(s):\n",
    "    \"\"\"\n",
    "    Given a string s, tokenize the string, then stem it.\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    word_tokenize(s)\n",
    "    return \" \".join([ps.stem(w) for w in word_tokenize(s)])\n",
    "\n",
    "token_stem(\"converts the words in word_tokens to lower case and then checks whether \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the tokenization and stemming to our lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seq'] = df['seq'].apply(token_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our lyrics text is ready for post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         no , no i is not ever trap out the bando but o...\n",
       "1         the drink go down and smoke goe up , i feel my...\n",
       "2         she do not live on planet earth no more she fo...\n",
       "3         trip off that grigio , mob , light low trip of...\n",
       "4         i see a midnight panther , so gallant and so b...\n",
       "                                ...                        \n",
       "158348    and we live on borrow time , but thi headshot ...\n",
       "158349    frozin in time forev carri that torch for so l...\n",
       "158350    hard to be a girl . so nice to be a boy . in m...\n",
       "158351    i want to chose to die , and be buri with a ru...\n",
       "158352    music ladder lean on mountain bath in white la...\n",
       "Name: seq, Length: 158353, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<158353x99073 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8188414 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the vectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    token_pattern=r'\\b[a-zA-Z]{3,}\\b', # we want only words that contain letters and are 3 or more characters long\n",
    ")\n",
    "\n",
    "# Transform our data into the document-term matrix\n",
    "dtm = vectorizer.fit_transform(df['seq'])\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aaa', 'aaaa', 'aaaaa', ..., 'zzznoahh', 'zzzz', 'zzzzzombiee'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99073,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abgefickt', 'abgehn', 'abgesaugt', 'abgestellt', 'abgrund',\n",
       "       'abgvll', 'abh', 'abhaya', 'abhor', 'abhorr', 'abi', 'abid',\n",
       "       'abidin', 'abierta', 'abierto', 'abigail', 'abil', 'abilen',\n",
       "       'abilin', 'abillatoeya', 'abillybopa', 'abilti', 'abireru',\n",
       "       'abismo', 'abit', 'abita', 'abitacion', 'abitai', 'abitar',\n",
       "       'abitch', 'abito', 'abituati', 'abitudin', 'abiyoyo', 'abizov',\n",
       "       'abject', 'abjur', 'abk', 'abl', 'ablaz', 'able', 'ablebodi',\n",
       "       'ableton', 'abli', 'ablig', 'ablow', 'ablut', 'abn', 'abneg',\n",
       "       'abner'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names[300:350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x99073 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 70 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = dtm[0]\n",
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act: 2\n",
      "attitud: 1\n",
      "bando: 1\n",
      "bodi: 1\n",
      "brand: 1\n",
      "bread: 1\n",
      "break: 1\n",
      "chang: 2\n",
      "coupl: 1\n",
      "crew: 1\n",
      "cut: 1\n",
      "die: 3\n",
      "drake: 1\n",
      "dream: 1\n",
      "dress: 1\n",
      "eat: 1\n",
      "everybodi: 1\n",
      "everyday: 45\n",
      "far: 1\n",
      "flex: 3\n",
      "fuck: 3\n",
      "funni: 1\n",
      "got: 3\n",
      "hard: 1\n",
      "heard: 1\n",
      "hobbi: 1\n",
      "hot: 1\n",
      "know: 4\n",
      "life: 3\n",
      "like: 3\n",
      "lone: 1\n",
      "lord: 1\n",
      "lose: 1\n",
      "love: 4\n",
      "man: 1\n",
      "money: 2\n",
      "motto: 1\n",
      "new: 1\n",
      "nigga: 8\n",
      "nobodi: 2\n",
      "ovo: 1\n",
      "place: 1\n",
      "plate: 1\n",
      "prolli: 1\n",
      "pull: 1\n",
      "pun: 1\n",
      "regular: 3\n",
      "rich: 3\n",
      "run: 1\n",
      "say: 2\n",
      "shit: 3\n",
      "shout: 1\n",
      "sinc: 1\n",
      "singer: 1\n",
      "sound: 1\n",
      "spectacular: 3\n",
      "spend: 3\n",
      "start: 2\n",
      "starv: 1\n",
      "stay: 1\n",
      "strang: 1\n",
      "swear: 2\n",
      "talk: 3\n",
      "trap: 1\n",
      "wanna: 1\n",
      "watch: 1\n",
      "whi: 1\n",
      "win: 1\n",
      "wrong: 1\n",
      "yeah: 3\n"
     ]
    }
   ],
   "source": [
    "row_index = 0\n",
    "doc_vec = dtm.getrow(row_index).toarray()\n",
    "\n",
    "non_zero_indices = doc_vec.nonzero()[1]\n",
    "dtm_scores = doc_vec[0, non_zero_indices] # goes and retrieves the values corresponding to the non_zero_indices\n",
    "words = [feature_names[i] for i in non_zero_indices]\n",
    "\n",
    "for word, score in zip(words, dtm_scores):\n",
    "    print(f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  778,  5043,  6389,  9715, 10697, 10810, 10823, 14424, 18595,\n",
       "       19060, 19896, 22649, 24611, 24682, 24761, 25673, 28121, 28127,\n",
       "       29114, 30628, 32182, 32431, 35053, 37188, 37767, 39125, 39814,\n",
       "       46577, 49098, 49218, 49991, 50168, 50210, 50314, 51716, 55677,\n",
       "       56316, 58451, 58716, 59106, 62116, 65318, 65451, 67505, 68061,\n",
       "       68117, 70432, 71538, 73121, 74428, 77241, 77539, 78176, 78207,\n",
       "       80507, 80842, 80907, 81924, 81946, 82023, 82694, 84406, 85063,\n",
       "       88768, 94700, 94947, 95752, 96322, 97219, 97727])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_zero_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.getcol(2327).toarray().T # get the column, turn it into an array format, then transpose it to be a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.getcol(44592).toarray().T # Fuck, which should appear often in songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(dtm.getcol(44592).toarray().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix2Doc(dtMatrix, features, index):\n",
    "    \"\"\"Turns each row of the document-term matrix into a list of terms\"\"\"\n",
    "    row = dtMatrix.getrow(index).toarray()\n",
    "    non_zero_indices = row.nonzero()[1]\n",
    "    words = [features[idx] for idx in non_zero_indices]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDocsAsTerms = [matrix2Doc(dtm, feature_names, i) for i in range(dtm.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158353"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allDocsAsTerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>artist</th>\n",
       "      <th>seq</th>\n",
       "      <th>song</th>\n",
       "      <th>label</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>no , no i is not ever trap out the bando but o...</td>\n",
       "      <td>Everyday</td>\n",
       "      <td>0.626</td>\n",
       "      <td>[act, attitud, bando, bodi, brand, bread, brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>the drink go down and smoke goe up , i feel my...</td>\n",
       "      <td>Live Till We Die</td>\n",
       "      <td>0.630</td>\n",
       "      <td>[away, band, bite, blow, care, chase, circl, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>she do not live on planet earth no more she fo...</td>\n",
       "      <td>The Otherside</td>\n",
       "      <td>0.240</td>\n",
       "      <td>[bad, bag, befor, broke, broken, caught, cheat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>trip off that grigio , mob , light low trip of...</td>\n",
       "      <td>Pinot</td>\n",
       "      <td>0.536</td>\n",
       "      <td>[alway, babi, becaus, begin, blow, boy, cheat,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>i see a midnight panther , so gallant and so b...</td>\n",
       "      <td>Shadows &amp; Diamonds</td>\n",
       "      <td>0.371</td>\n",
       "      <td>[alon, answer, believ, brave, broke, coal, cof...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        artist  \\\n",
       "0           0  Elijah Blake   \n",
       "1           1  Elijah Blake   \n",
       "2           2  Elijah Blake   \n",
       "3           3  Elijah Blake   \n",
       "4           4  Elijah Blake   \n",
       "\n",
       "                                                 seq                song  \\\n",
       "0  no , no i is not ever trap out the bando but o...            Everyday   \n",
       "1  the drink go down and smoke goe up , i feel my...    Live Till We Die   \n",
       "2  she do not live on planet earth no more she fo...       The Otherside   \n",
       "3  trip off that grigio , mob , light low trip of...               Pinot   \n",
       "4  i see a midnight panther , so gallant and so b...  Shadows & Diamonds   \n",
       "\n",
       "   label                                              terms  \n",
       "0  0.626  [act, attitud, bando, bodi, brand, bread, brea...  \n",
       "1  0.630  [away, band, bite, blow, care, chase, circl, c...  \n",
       "2  0.240  [bad, bag, befor, broke, broken, caught, cheat...  \n",
       "3  0.536  [alway, babi, becaus, begin, blow, boy, cheat,...  \n",
       "4  0.371  [alon, answer, believ, brave, broke, coal, cof...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['terms'] = allDocsAsTerms\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=15, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LatentDirichletAllocation<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\">?<span>Documentation for LatentDirichletAllocation</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LatentDirichletAllocation(n_components=15, random_state=0)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=15, random_state=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Step 1: Initialize the model\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=15, # we are picking the number of topics arbitrarely at the moment\n",
    "                                random_state=0)\n",
    "\n",
    "# Step 2: Fit the model\n",
    "lda.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.68600788, 0.06666695, 0.06666748, ..., 0.06666667, 0.06666667,\n",
       "        0.06666667],\n",
       "       [0.066667  , 0.06666725, 0.06666667, ..., 0.06666682, 0.06666725,\n",
       "        0.06666695],\n",
       "       [0.06666683, 1.99695389, 0.06666672, ..., 0.06666667, 0.06666667,\n",
       "        0.06666667],\n",
       "       ...,\n",
       "       [0.06666678, 0.06666674, 0.06666667, ..., 0.06666667, 0.06666669,\n",
       "        0.06666671],\n",
       "       [0.06666672, 1.37573704, 0.06666667, ..., 0.06666667, 0.06666675,\n",
       "        1.06666115],\n",
       "       [6.21954668, 1.85231007, 0.06666667, ..., 0.06666667, 2.066665  ,\n",
       "        0.06666728]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 99073)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "pum doo dem mari boom beat yuh rum nah america\n",
      "Topic 1:\n",
      "nigga fuck got like shit bitch thi money know ass\n",
      "Topic 2:\n",
      "lord good ride tonight gon jesu alright sing king got\n",
      "Topic 3:\n",
      "rock got new like work just play roll thi bye\n",
      "Topic 4:\n",
      "come day away home night time sun walk littl morn\n",
      "Topic 5:\n",
      "love dream heart song sing long old just night kiss\n",
      "Topic 6:\n",
      "blue cri fli high sky whi lone mama die happi\n",
      "Topic 7:\n",
      "run que wild ladi devil whi sha tiger amor soul\n",
      "Topic 8:\n",
      "like just onli believ make thing said head smile eye\n",
      "Topic 9:\n",
      "love babi want know let just feel yeah need got\n",
      "Topic 10:\n",
      "know time just thi say think feel tri way want\n",
      "Topic 11:\n",
      "yeah got girl like gon know let come danc littl\n",
      "Topic 12:\n",
      "thi let god eye soul burn life come feel light\n",
      "Topic 13:\n",
      "world live life hey thi look way peopl turn round\n",
      "Topic 14:\n",
      "man like woman got gon gun just kid said run\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, features, no_top_words):\n",
    "    \"\"\"Helper function to show the top words of a model\"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([features[i]\n",
    "                        for i in topic.argsort()[:-no_top_words-1:-1]])) # syntax for reversing a list [::-1]\n",
    "\n",
    "display_topics(lda, feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.14078826e-04, 7.14970740e-01, 4.14081531e-04, ...,\n",
       "        4.14079168e-04, 7.36064008e-02, 4.14079439e-04],\n",
       "       [3.00300487e-04, 8.21262441e-02, 3.00300738e-04, ...,\n",
       "        3.00301014e-04, 5.05506867e-01, 3.00300814e-04],\n",
       "       [6.80272233e-04, 1.73118525e-01, 6.80272906e-04, ...,\n",
       "        6.80273685e-04, 1.06140534e-01, 6.80274272e-04],\n",
       "       ...,\n",
       "       [1.75438686e-03, 1.75438899e-03, 4.48299426e-02, ...,\n",
       "        1.75439384e-03, 1.75438897e-03, 1.08689853e-01],\n",
       "       [1.23456949e-03, 1.70729568e-01, 1.23457064e-03, ...,\n",
       "        1.92636023e-01, 1.23457129e-03, 2.84377936e-01],\n",
       "       [1.19047677e-03, 1.19047998e-03, 9.44322446e-02, ...,\n",
       "        1.19048014e-03, 6.72087154e-02, 2.26268293e-01]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_dist = lda.transform(dtm)\n",
    "doc_topic_dist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158353, 15)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayHeader(model, features, no_top_words):\n",
    "    \"\"\"Helper function to show the top words of a model\"\"\"\n",
    "    topicNames = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topicNames.append(f\"Topic {topic_idx}: \" + (\", \".join([features[i]\n",
    "                             for i in topic.argsort()[:-no_top_words-1:-1]])))\n",
    "    return topicNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0: pum, doo, dem, mari, boom</th>\n",
       "      <th>Topic 1: nigga, fuck, got, like, shit</th>\n",
       "      <th>Topic 2: lord, good, ride, tonight, gon</th>\n",
       "      <th>Topic 3: rock, got, new, like, work</th>\n",
       "      <th>Topic 4: come, day, away, home, night</th>\n",
       "      <th>Topic 5: love, dream, heart, song, sing</th>\n",
       "      <th>Topic 6: blue, cri, fli, high, sky</th>\n",
       "      <th>Topic 7: run, que, wild, ladi, devil</th>\n",
       "      <th>Topic 8: like, just, onli, believ, make</th>\n",
       "      <th>Topic 9: love, babi, want, know, let</th>\n",
       "      <th>Topic 10: know, time, just, thi, say</th>\n",
       "      <th>Topic 11: yeah, got, girl, like, gon</th>\n",
       "      <th>Topic 12: thi, let, god, eye, soul</th>\n",
       "      <th>Topic 13: world, live, life, hey, thi</th>\n",
       "      <th>Topic 14: man, like, woman, got, gon</th>\n",
       "      <th>dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.172</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic 0: pum, doo, dem, mari, boom  Topic 1: nigga, fuck, got, like, shit  \\\n",
       "0                               0.000                                  0.715   \n",
       "1                               0.000                                  0.082   \n",
       "2                               0.001                                  0.173   \n",
       "3                               0.000                                  0.237   \n",
       "4                               0.111                                  0.001   \n",
       "\n",
       "   Topic 2: lord, good, ride, tonight, gon  \\\n",
       "0                                    0.000   \n",
       "1                                    0.000   \n",
       "2                                    0.001   \n",
       "3                                    0.000   \n",
       "4                                    0.001   \n",
       "\n",
       "   Topic 3: rock, got, new, like, work  Topic 4: come, day, away, home, night  \\\n",
       "0                                0.000                                  0.000   \n",
       "1                                0.000                                  0.000   \n",
       "2                                0.001                                  0.001   \n",
       "3                                0.000                                  0.057   \n",
       "4                                0.001                                  0.001   \n",
       "\n",
       "   Topic 5: love, dream, heart, song, sing  \\\n",
       "0                                    0.000   \n",
       "1                                    0.000   \n",
       "2                                    0.457   \n",
       "3                                    0.000   \n",
       "4                                    0.026   \n",
       "\n",
       "   Topic 6: blue, cri, fli, high, sky  Topic 7: run, que, wild, ladi, devil  \\\n",
       "0                               0.000                                 0.000   \n",
       "1                               0.246                                 0.000   \n",
       "2                               0.001                                 0.001   \n",
       "3                               0.000                                 0.000   \n",
       "4                               0.054                                 0.001   \n",
       "\n",
       "   Topic 8: like, just, onli, believ, make  \\\n",
       "0                                    0.000   \n",
       "1                                    0.000   \n",
       "2                                    0.046   \n",
       "3                                    0.000   \n",
       "4                                    0.001   \n",
       "\n",
       "   Topic 9: love, babi, want, know, let  Topic 10: know, time, just, thi, say  \\\n",
       "0                                 0.117                                 0.090   \n",
       "1                                 0.000                                 0.000   \n",
       "2                                 0.001                                 0.091   \n",
       "3                                 0.089                                 0.364   \n",
       "4                                 0.299                                 0.186   \n",
       "\n",
       "   Topic 11: yeah, got, girl, like, gon  Topic 12: thi, let, god, eye, soul  \\\n",
       "0                                 0.000                               0.000   \n",
       "1                                 0.163                               0.000   \n",
       "2                                 0.120                               0.001   \n",
       "3                                 0.249                               0.000   \n",
       "4                                 0.001                               0.143   \n",
       "\n",
       "   Topic 13: world, live, life, hey, thi  \\\n",
       "0                                  0.074   \n",
       "1                                  0.506   \n",
       "2                                  0.106   \n",
       "3                                  0.000   \n",
       "4                                  0.001   \n",
       "\n",
       "   Topic 14: man, like, woman, got, gon  dominant_topic  \n",
       "0                                 0.000               1  \n",
       "1                                 0.000              13  \n",
       "2                                 0.001               5  \n",
       "3                                 0.000              10  \n",
       "4                                 0.172               9  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column names\n",
    "topicnames = displayHeader(lda, feature_names, 5)\n",
    "\n",
    "# index names\n",
    "docnames = df.index.tolist() # We will use the original names of the documents\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(doc_topic_dist, 3), \n",
    "                                 columns=topicnames, \n",
    "                                 index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1) # finds the maximum argument\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "df_document_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0: pum, doo, dem, mari, boom</th>\n",
       "      <th>Topic 1: nigga, fuck, got, like, shit</th>\n",
       "      <th>Topic 2: lord, good, ride, tonight, gon</th>\n",
       "      <th>Topic 3: rock, got, new, like, work</th>\n",
       "      <th>Topic 4: come, day, away, home, night</th>\n",
       "      <th>Topic 5: love, dream, heart, song, sing</th>\n",
       "      <th>Topic 6: blue, cri, fli, high, sky</th>\n",
       "      <th>Topic 7: run, que, wild, ladi, devil</th>\n",
       "      <th>Topic 8: like, just, onli, believ, make</th>\n",
       "      <th>Topic 9: love, babi, want, know, let</th>\n",
       "      <th>Topic 10: know, time, just, thi, say</th>\n",
       "      <th>Topic 11: yeah, got, girl, like, gon</th>\n",
       "      <th>Topic 12: thi, let, god, eye, soul</th>\n",
       "      <th>Topic 13: world, live, life, hey, thi</th>\n",
       "      <th>Topic 14: man, like, woman, got, gon</th>\n",
       "      <th>dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.001</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.112</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic 0: pum, doo, dem, mari, boom  Topic 1: nigga, fuck, got, like, shit  \\\n",
       "76                               0.001                                  0.036   \n",
       "77                               0.001                                  0.001   \n",
       "78                               0.001                                  0.250   \n",
       "79                               0.001                                  0.001   \n",
       "80                               0.001                                  0.001   \n",
       "81                               0.000                                  0.243   \n",
       "82                               0.000                                  0.000   \n",
       "83                               0.000                                  0.000   \n",
       "84                               0.001                                  0.330   \n",
       "85                               0.001                                  0.001   \n",
       "\n",
       "    Topic 2: lord, good, ride, tonight, gon  \\\n",
       "76                                    0.028   \n",
       "77                                    0.001   \n",
       "78                                    0.001   \n",
       "79                                    0.001   \n",
       "80                                    0.001   \n",
       "81                                    0.000   \n",
       "82                                    0.000   \n",
       "83                                    0.033   \n",
       "84                                    0.001   \n",
       "85                                    0.001   \n",
       "\n",
       "    Topic 3: rock, got, new, like, work  \\\n",
       "76                                0.558   \n",
       "77                                0.077   \n",
       "78                                0.001   \n",
       "79                                0.001   \n",
       "80                                0.001   \n",
       "81                                0.191   \n",
       "82                                0.560   \n",
       "83                                0.000   \n",
       "84                                0.001   \n",
       "85                                0.001   \n",
       "\n",
       "    Topic 4: come, day, away, home, night  \\\n",
       "76                                  0.001   \n",
       "77                                  0.001   \n",
       "78                                  0.001   \n",
       "79                                  0.050   \n",
       "80                                  0.027   \n",
       "81                                  0.000   \n",
       "82                                  0.000   \n",
       "83                                  0.024   \n",
       "84                                  0.001   \n",
       "85                                  0.001   \n",
       "\n",
       "    Topic 5: love, dream, heart, song, sing  \\\n",
       "76                                    0.001   \n",
       "77                                    0.110   \n",
       "78                                    0.001   \n",
       "79                                    0.178   \n",
       "80                                    0.001   \n",
       "81                                    0.000   \n",
       "82                                    0.159   \n",
       "83                                    0.137   \n",
       "84                                    0.001   \n",
       "85                                    0.001   \n",
       "\n",
       "    Topic 6: blue, cri, fli, high, sky  Topic 7: run, que, wild, ladi, devil  \\\n",
       "76                               0.001                                 0.040   \n",
       "77                               0.001                                 0.001   \n",
       "78                               0.001                                 0.001   \n",
       "79                               0.065                                 0.001   \n",
       "80                               0.001                                 0.001   \n",
       "81                               0.000                                 0.000   \n",
       "82                               0.000                                 0.000   \n",
       "83                               0.048                                 0.033   \n",
       "84                               0.001                                 0.001   \n",
       "85                               0.001                                 0.001   \n",
       "\n",
       "    Topic 8: like, just, onli, believ, make  \\\n",
       "76                                    0.001   \n",
       "77                                    0.001   \n",
       "78                                    0.001   \n",
       "79                                    0.001   \n",
       "80                                    0.001   \n",
       "81                                    0.047   \n",
       "82                                    0.115   \n",
       "83                                    0.000   \n",
       "84                                    0.072   \n",
       "85                                    0.001   \n",
       "\n",
       "    Topic 9: love, babi, want, know, let  \\\n",
       "76                                 0.001   \n",
       "77                                 0.001   \n",
       "78                                 0.001   \n",
       "79                                 0.119   \n",
       "80                                 0.040   \n",
       "81                                 0.402   \n",
       "82                                 0.000   \n",
       "83                                 0.000   \n",
       "84                                 0.001   \n",
       "85                                 0.759   \n",
       "\n",
       "    Topic 10: know, time, just, thi, say  \\\n",
       "76                                 0.260   \n",
       "77                                 0.595   \n",
       "78                                 0.166   \n",
       "79                                 0.579   \n",
       "80                                 0.131   \n",
       "81                                 0.000   \n",
       "82                                 0.000   \n",
       "83                                 0.098   \n",
       "84                                 0.542   \n",
       "85                                 0.142   \n",
       "\n",
       "    Topic 11: yeah, got, girl, like, gon  Topic 12: thi, let, god, eye, soul  \\\n",
       "76                                 0.001                               0.073   \n",
       "77                                 0.198                               0.001   \n",
       "78                                 0.574                               0.001   \n",
       "79                                 0.001                               0.001   \n",
       "80                                 0.163                               0.001   \n",
       "81                                 0.000                               0.000   \n",
       "82                                 0.162                               0.000   \n",
       "83                                 0.290                               0.235   \n",
       "84                                 0.049                               0.001   \n",
       "85                                 0.001                               0.091   \n",
       "\n",
       "    Topic 13: world, live, life, hey, thi  \\\n",
       "76                                  0.001   \n",
       "77                                  0.015   \n",
       "78                                  0.001   \n",
       "79                                  0.001   \n",
       "80                                  0.633   \n",
       "81                                  0.000   \n",
       "82                                  0.000   \n",
       "83                                  0.000   \n",
       "84                                  0.001   \n",
       "85                                  0.001   \n",
       "\n",
       "    Topic 14: man, like, woman, got, gon  dominant_topic  \n",
       "76                                 0.001               3  \n",
       "77                                 0.001              10  \n",
       "78                                 0.001              11  \n",
       "79                                 0.001              10  \n",
       "80                                 0.001              13  \n",
       "81                                 0.112               9  \n",
       "82                                 0.000               3  \n",
       "83                                 0.100              11  \n",
       "84                                 0.001              10  \n",
       "85                                 0.001               9  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_document_topic[76:86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic Num</th>\n",
       "      <th>Num Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>43417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>21960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>17647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>12412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>8757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>8721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>7296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>6983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>6087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>5477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>3263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>2553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>1144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic Num  Num Documents\n",
       "0          10          43417\n",
       "1          12          21960\n",
       "2           9          17647\n",
       "3           5          12412\n",
       "4           4          11316\n",
       "5          11           8757\n",
       "6           3           8721\n",
       "7          14           7296\n",
       "8           1           6983\n",
       "9           8           6087\n",
       "10         13           5477\n",
       "11          6           3263\n",
       "12          2           2553\n",
       "13          0           1320\n",
       "14          7           1144"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
    "df_topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/stemmed_cleaned_lyrics.csv')\n",
    "df_document_topic.to_csv('lda-results/document_topic.csv')\n",
    "df_topic_distribution.to_csv('lda-results/topic_distribution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(lda, param_grid\u001b[38;5;241m=\u001b[39msearch_params)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Do the Grid Search\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:893\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_train \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 893\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/sklearn/decomposition/_lda.py:673\u001b[0m, in \u001b[0;36mLatentDirichletAllocation.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_em_step(\n\u001b[1;32m    666\u001b[0m             X[idx_slice, :],\n\u001b[1;32m    667\u001b[0m             total_samples\u001b[38;5;241m=\u001b[39mn_samples,\n\u001b[1;32m    668\u001b[0m             batch_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    669\u001b[0m             parallel\u001b[38;5;241m=\u001b[39mparallel,\n\u001b[1;32m    670\u001b[0m         )\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;66;03m# batch update\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_em_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;66;03m# check perplexity\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_every \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m evaluate_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/sklearn/decomposition/_lda.py:524\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._em_step\u001b[0;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"EM update for 1 iteration.\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03mupdate `_component` by batch VB or online VB.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;124;03m    Unnormalized document topic distribution.\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# E-step\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m _, suff_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;66;03m# M-step\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_update:\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/sklearn/decomposition/_lda.py:467\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._e_step\u001b[0;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parallel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 467\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_update_doc_distribution\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp_dirichlet_component_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc_topic_prior_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_doc_update_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_change_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# merge result\u001b[39;00m\n\u001b[1;32m    481\u001b[0m doc_topics, sstats_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CS315-Final-Project/.venv/lib/python3.11/site-packages/sklearn/decomposition/_lda.py:150\u001b[0m, in \u001b[0;36m_update_doc_distribution\u001b[0;34m(X, exp_topic_word_distr, doc_topic_prior, max_doc_update_iter, mean_change_tol, cal_sstats, random_state)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# Note: adds doc_topic_prior to doc_topic_d, in-place.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     dirichlet_expectation_1d(doc_topic_d, doc_topic_prior, exp_doc_topic_d)\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmean_change\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_topic_d\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m mean_change_tol:\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    152\u001b[0m doc_topic_distr[idx_d, :] \u001b[38;5;241m=\u001b[39m doc_topic_d\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# We are going to test multiple values for the number of topics\n",
    "search_params = {'n_components': [5, 10, 15, 20, 25, 30, 35]}\n",
    "\n",
    "# Initialize the LDA model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Initialize a Grid Search with cross-validation instance\n",
    "grid = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "grid.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model\n",
    "best_lda_model = grid.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", grid.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", grid.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {'n_components': [1,2,3,4,5,6]}\n",
    "\n",
    "lda = LatentDirichletAllocation()\n",
    "grid = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "grid.fit(dtm)\n",
    "\n",
    "# Best Model\n",
    "best_lda_model = grid.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", grid.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", grid.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(best_lda_model, feature_names, 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
